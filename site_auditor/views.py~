from django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import render_to_response
from link_check_tool.models import Site, Link, Page, H1, H2, Test_Date
from forms import SiteForm
from django.core.context_processors import csrf


def linkchecks_submit(request):
    if request.GET:
        form = SiteForm(request.GET)
        if form.is_valid():
            form.save()
            return HttpResponseRedirect('/tools/linkchecks/result/')
    else:
        form = SiteForm()

    args = {}
    args.update(csrf(request))
    args['form'] = form
    return render_to_response('submit.html', args)
    
    
def linkchecks_result(request):
    crawl(request.GET['url'])
    return render_to_response('result.html', {'page_archive': page_archive})



#######################################################################################
# BELOW ARE THE CODES FOR INTERNAL LINK CRAWLER

main_site_canonical_url = ''
unvisited_page_urls = []
page_archive = {}
crawl_ended = False
static_file_types = ['.png', '.jpg', 'jpeg', '.gif', '.svg', '.mp3', '.mp4', '.css']

class Page:
    def __init__(self, main_site_canonical_url):
        self.main_site_canonical_url = main_site_canonical_url

        self.h1_list = []
        self.h2_list = []
        self.h3_list = []
        self.h4_list = []
        self.h5_list = []
        self.h6_list = []

        self.live_img_links = {}
        self.live_css_links = []
        self.live_js_links = []
        self.live_page_links = [] # links to other pages that are contained in this page                                                                                                  
        self.false_img_links = []
        self.false_css_links = []
        self.false_js_links = []
        self.false_page_links = [] # broken links; links that point to nowhere; bad for SEO                                                                                               

        self.listed_canonical_links = []
        self.trivial_links = []
        self.n = 0 # number of occurence this particular page URL has been used in the site                                                                                               

    def __str__(self):
        return self.url

# this code was copied from Stack Overflow; http://stackoverflow.com/questions/5538280/determining-redirected-url-in-python                                                               
def get_redirected_url(url):
    opener = build_opener(HTTPRedirectHandler)
    request = opener.open(url)
    return request.url

# set URL provided by user site's main URL only if URL input is not empty and is valid                                                                                                    
def set_main_url(url):
    global main_site_canonical_url
    if is_valid_url(url):
        main_site_canonical_url = get_redirected_url(url)
        print 'main site canonical URL set to ' + url
    else:
        print 'main site canonical URL not set'

def is_valid_url(url):
    if 'http://' in url:
        try:
            status_code = head(url).status_code
        except:
            status_code = False
        print url + ' valid URL'
        return status_code != 404
    else:
        print url + ' invalid URL'
        return False

def load_html(url):
    try:
        html = urlopen(url)
    except:
        print 'Could not load: ' + url
        return False
    html = BeautifulSoup(html)
    return html

def initialize(url):
    global page_archive
    if is_valid_url(url):
        set_main_url(url)
        html = load_html(main_site_canonical_url)
        if html:
            process_page(main_site_canonical_url, html)
        else:
            print 'failed to initialize'

def extract_text(raw_html):
    return ''.join(raw_html.findAll(text=True)).strip()

def process_page(url, html):
    h1_list = html.findAll('h1')
    h2_list = html.findAll('h2')
    h3_list = html.findAll('h3')
    h4_list = html.findAll('h4')
    h5_list = html.findAll('h5')
    h6_list = html.findAll('h6')
    img_list = html.findAll('img', src=True)
    css_list = html.findAll('link', rel='stylesheet')
    js_list = html.findAll('script', src=True)
    canonical_list = html.findAll('link', rel='canonical')
    a_list = html.findAll('a', href=True)

    page_archive[url] = Page(url)
    # process h1 tags; extract text for h1 tags from wrapping tags                                                                                                                        
    for h1 in h1_list:
        page_archive[url].h1_list.append(extract_text(h1))
    print '\nh1 instances:'
    print page_archive[url].h1_list

    # process h2 tags; extract text for h2 tags from wrapping tags                                                                                                                        
    for h2 in h2_list:
        page_archive[url].h2_list.append(extract_text(h2))
    print '\nh2 instances:'
    print page_archive[url].h2_list

    # process h3 tags; extract text for h3 tags from wrapping tags                                                                                                                        
    for h3 in h3_list:
        page_archive[url].h3_list.append(extract_text(h3))
    print '\nh3 instances:'
    print page_archive[url].h3_list

    # process h4 tags; extract text for h3 tags from wrapping tags                                                                                                                        
    for h4 in h4_list:
        page_archive[url].h4_list.append(extract_text(h4))
    print '\nh4 instances:'
    print page_archive[url].h4_list

  # process h5 tags; extract text for h5 tags from wrapping tags                                                                                                                        
    for h5 in h5_list:
        page_archive[url].h5_list.append(extract_text(h5))
    print '\nh5 instances:'
    print page_archive[url].h5_list

    # process h6 tags; extract text for h6 tags from wrapping tags                                                                                                                        
    for h6 in h6_list:
        page_archive[url].h6_list.append(extract_text(h6))
    print '\nh6 instances:'
    print page_archive[url].h6_list

    # process image links                                                                                                                                                                 
    for img in img_list:
        img_link = img['src']
        img_alt = img['alt']
        img_found = head(img_link).status_code != 404
        if img_found:
            page_archive[url].live_img_links[img_link] = 'Alt description: ' + img_alt
        else:
            page_archive[url].false_img_links.append(img_link)
    print '\nlive image links:'
    print page_archive[url].live_img_links

   # process css links                                                                                                                                                                   
    for css in css_list:
        css_link = css['href']
        css_found = head(css_link).status_code != 404
        if css_found:
            page_archive[url].live_css_links.append(css_link)
        else:
            page_archive[url].false_css_links.append(css_link)
    print '\nlive css links:'
    print page_archive[url].live_css_links

    # process js links                                                                                                                                                                    
    for js in js_list:
        js_link = js['src']
        js_found = head(js_link).status_code != 404
        if js_found:
            page_archive[url].live_js_links.append(js_link)
        else:
            page_archive[url].false_js_links.append(js_link)
    print '\nlive js links:'
    print page_archive[url].live_js_links.append(js_link)

    # process canonicalization                                                                                                                                                            
    for c_link in canonical_list:
        page_archive[url].listed_canonical_links.append(c_link['href'])
    print '\nlisted canonical links:'
    print page_archive[url].listed_canonical_links

    # process page links                                                                                                                                                                  
    for a in a_list:
        a['href'] = a_href = urljoin(main_site_canonical_url, a['href'])
        if main_site_canonical_url in a_href and a_href not in page_archive and a_href not in unvisited_page_urls:
            if '#' not in a_href:
                link_active = head(a_href).status_code.status_code != 404
                if link_active:
                    page_archive[url].live_page_links.append(a_href)
                    unvisited_page_urls.append(a_href)
                else:
                    page_archive[url].false_page_links.append(a_href)
            else:
                page_archive[url].trivial_links.append(a_href)

    print '\nlive page links:'
    print page_archive[url].live_page_links
    print '\nfalse page links:'
    print page_archive[url].false_page_links
    print '\ntrivial links:'
    print page_archive[url].trivial_links
    print '\nunvisited page links:'
    print unvisited_page_urls

def crawl(url):
    initialize(url)
    while len(unvisited_page_urls) > 0:
        html = load_html(unvisited_page_urls[0])
        process_page(unvisited_page_urls[0], html)
        print len(unvisited_page_urls)
        unvisited_page_urls.pop(0)

